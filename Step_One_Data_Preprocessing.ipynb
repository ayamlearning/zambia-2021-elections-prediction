{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "mount_file_id": "1ed916N2_gNEGdlyqNniBUXvz71vqhW5Q",
      "authorship_tag": "ABX9TyOCU8MzB3HyvPc7f2EJh+LX",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayamlearning/zambia-2021-elections-prediction/blob/main/Step_One_Data_Preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8zkza95_GO4G"
      },
      "outputs": [],
      "source": [
        "!pip install emoji\n",
        "!pip install contractions\n",
        "!pip install inflect"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re, string, unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "import inflect\n",
        "from nltk import word_tokenize, sent_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import LancasterStemmer, WordNetLemmatizer\n",
        "import pandas as pd\n",
        "import emoji\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "ChKVxFheJMdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=pd.read_csv('/content/drive/MyDrive/Masters Program/data/sample.csv',\n",
        "               index_col=0)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "K4LWqJnfHhZH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_URL(sample):\n",
        "    \"\"\"Remove URLs from a sample string\"\"\"\n",
        "    return re.sub(r\"http\\S+\", \"\", sample)"
      ],
      "metadata": {
        "id": "pa40djwbIhQG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_emojis_to_word(text):\n",
        "    \"\"\"Removing emojis from the text\"\"\"\n",
        "    return emoji.demojize(text)"
      ],
      "metadata": {
        "id": "E8f_wVTUcs4Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_contractions(text):\n",
        "    \"\"\"Replace contractions in string of text\"\"\"\n",
        "    return contractions.fix(text)"
      ],
      "metadata": {
        "id": "uyrMN63Bcigv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_non_ascii(words):\n",
        "    \"\"\"Remove non-ASCII characters from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = unicodedata.normalize('NFKD', word).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "kRyNnXMZb0Jx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def to_lowercase(words):\n",
        "    \"\"\"Convert all characters to lowercase from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = word.lower()\n",
        "        new_words.append(new_word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "bVrB9RxAb7BX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_punctuation(words):\n",
        "    \"\"\"Remove punctuation from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        new_word = re.sub(r'[^\\w\\s]', '', word)\n",
        "        if new_word != '':\n",
        "            new_words.append(new_word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "YoM2luPJcBu7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def replace_numbers(words):\n",
        "    \"\"\"Replace all interger occurrences in list of tokenized words with textual representation\"\"\"\n",
        "    p = inflect.engine()\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word.isdigit():\n",
        "            new_word = p.number_to_words(word)\n",
        "            new_words.append(new_word)\n",
        "        else:\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "xhoeN8vmcF53"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(words):\n",
        "    \"\"\"Remove stop words from list of tokenized words\"\"\"\n",
        "    new_words = []\n",
        "    for word in words:\n",
        "        if word not in stopwords.words('english'):\n",
        "            new_words.append(word)\n",
        "    return new_words"
      ],
      "metadata": {
        "id": "SOcqiaZwcLXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stem_words(words):\n",
        "    \"\"\"Stem words in list of tokenized words\"\"\"\n",
        "    stemmer = LancasterStemmer()\n",
        "    stems = []\n",
        "    for word in words:\n",
        "        stem = stemmer.stem(word)\n",
        "        stems.append(stem)\n",
        "    return stems"
      ],
      "metadata": {
        "id": "Qeglf7FfcP8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatize_verbs(words):\n",
        "    \"\"\"Lemmatize verbs in list of tokenized words\"\"\"\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmas = []\n",
        "    for word in words:\n",
        "        lemma = lemmatizer.lemmatize(word, pos='v')\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas"
      ],
      "metadata": {
        "id": "m3V1Sk1ycVDk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_mentions_and_hashtags(sample):\n",
        "  sample = re.sub(\"@[A-Za-z0-9_]+\",\"\", sample)\n",
        "  sample = re.sub(\"#[A-Za-z0-9_]+\",\"\", sample)\n",
        "  return sample"
      ],
      "metadata": {
        "id": "5mZ6kdKNglD1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def normalize(words):\n",
        "    words = remove_non_ascii(words)\n",
        "    words = to_lowercase(words)\n",
        "    #words = remove_punctuation(words)\n",
        "    words = replace_numbers(words)\n",
        "    #words = remove_stopwords(words)\n",
        "    return \" \".join(words)"
      ],
      "metadata": {
        "id": "5h-v8j97PBvg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(sample):\n",
        "    sample = remove_URL(sample)\n",
        "    sample = replace_contractions(sample)\n",
        "    sample = convert_emojis_to_word(sample)\n",
        "    sample = remove_mentions_and_hashtags(sample)\n",
        "    # Tokenize\n",
        "    words = nltk.word_tokenize(sample)\n",
        "    # Normalize\n",
        "    return normalize(words)"
      ],
      "metadata": {
        "id": "VQ6grUI1cb2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "preprocess(\"\"\"\n",
        "Our very own sweet Manana üòçüòç\n",
        "Zambia‚Äôs First Lady @MrsHichilema \n",
        "#ZambiaDecides2021 #ZambiaDecided https://t.co/e9rh6DwdZU\"\"\") \n",
        "  \n",
        "\n"
      ],
      "metadata": {
        "id": "RsThP554XWg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['text']=df['text'].apply(preprocess)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "kO75IjFJfkgh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv('/content/drive/MyDrive/Masters Program/data/clean_sample.csv')"
      ],
      "metadata": {
        "id": "3kVecpqaik4x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M4MVFrjWjC2u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}